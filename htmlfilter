#!/usr/bin/env python

from bs4 import BeautifulSoup, Comment
import html5lib
import sys
import re

html_doc = sys.stdin.read()

# Code based off of: http://blog.odonnell.nu/posts/html-comments/

def sanitize(html):  
      
    if not html:  
        return  
  
    # Completely remove these tags and their contents, to be safe
    blacklist = ["script", "style" ]  
    
    # Allow these tags
    # NOTE: class, name, id are not included b/c they
    # are technically not needed for display purposes so it
    # is simpler just to ignore them
    whitelist = [  
        "div", "span", "p", "br", "pre",  
        "table", "tbody", "thead", "tr", "td", "a",
        "ul", "li", "ol",   
        "b", "em", "i", "strong", "u", "font",
        "img"
        ]  

    attr_whitelist = { 'a':['href','title','hreflang'], 'p':[], 'img':['src', 'width', 'height', 'alt', 'title'] }
    
    attributes_with_urls = [ 'href', 'src' ]
    
    soup = BeautifulSoup(html, "html5lib")           
  
    # Sanitize HTML by removing things that are potentially dangerous
    for tag in soup.findAll():  
        if tag.name.lower() in blacklist:   
            tag.extract()  # completely remove blacklisted tags
        elif tag.name.lower() in whitelist:  # names are like 'a', 'p', etc.
            # Tag is allowed so make sure all the attributes are allowed.

            # NOTE: use an array to keep track of bad attr's, then remove them
            # from the tag.attrs dictionary AFTER the loop so no "change size" error
            bad_attrs = []
            for attr in tag.attrs: # attrs are like name, class, href, etc.
                # allowed attributes are whitelisted per-tag
                if tag.name.lower() in attr_whitelist and attr.lower() in attr_whitelist[ tag.name.lower() ]:
                    # Check for safe URLs. Don't want javascript, etc.
                    if attr.lower() in attributes_with_urls:
                        # NOTE: we currently don't support relative URLs, so they
                        # just get removed for simplicity
                        if not re.match(r'(https?|ftp)://', tag.attrs[attr].lower()):
                            bad_attrs += [attr] # TODO: fix and uncomment
                            pass
                else:
                    # Remove tags that are not whitelisted
                    # Note: includes 'class', 'id', 'name' atm as described above
                    bad_attrs += [attr]
            # Remove the bad attrs from the tag
            for ba in bad_attrs:
                del tag.attrs[ba]
        else:  
            # Tags that are not whitelisted nor blacklisted are
            # turned into empty span tags
            tag.name = "span" 
            #tag.hidden = True # Note: can uncomment this if just want text and no span tag
            tag.attrs = []  
  
    # Just remove all comments just in case they have scripts in them
    comments = soup.findAll(text=lambda text:isinstance(text, Comment))  
    for comment in comments:  
        comment.extract()  
  
    safe_html = unicode(soup)  
    return safe_html


print sanitize(html_doc)
