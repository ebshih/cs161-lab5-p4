#!/usr/bin/env python



# TODO: get this input thing working, then can use HTML.txt from cmd
import sys
html_file_name = sys.argv[1]
with open(html_file_name) as html_file:
    html_doc = html_file.read()

print "Here is unsanitized HTML,"
print html_doc
print "\n"



#html_doc = """
#<html><head><title>The Dormouse's story</title></head>
#
#<p class="title"><b>The Dormouse's story</b></p>
#
#<p class="story">Once upon a time there were three little sisters; and their names were
#<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
#<a hrefz="http://example.com/lacie" class="sister" id="link2" name="Lacie">Lacie</a> and
#<a href="httpz://example.com/tillie" class="sister" id="link3">Tillie</a>;
#<BAD> can we get this text to show up!!!! </BAD>
#<img src="smiley.gif" alt="Smiley face" height="42" width="42" poop="3"/> 
#<img src="http://example.com/smiley.gif" alt="Smiley face 2" height="42" width="42"/>
#<script>alert("XSS")</script>
#<!--This is a comment. Comments are not displayed in the browser-->
#
#and they lived at the bottom of a well.</p>
#
#<p class="story">...</p>
#"""

# TODO: add : <IMG """><SCRIPT>alert("XSS")</SCRIPT>">
# maybe test different html parser/

# Code based off of: http://blog.odonnell.nu/posts/html-comments/

from bs4 import BeautifulSoup, Comment
import re #regex

def sanitize(html):  
      
    if not html:  
        return  
  
    # remove these tags, complete with contents.  
    # TODO: do we want the contents to come through, like as plain text?"
    blacklist = ["script", "style" ]  
    
    # allow these tags. NOTE: class, name, id are not included b/c
    # they are technically not needed for display purposes so it
    # is simpler just to ignore them
    whitelist = [  
        "div", "span", "p", "br", "pre",  
        "table", "tbody", "thead", "tr", "td", "a",
        "ul", "li", "ol",   
        "b", "em", "i", "strong", "u", "font",
        "img"
        ]  
    # TODO: write code to sanitize img tags
    attr_whitelist = { 'a':['href','title','hreflang'], 'p':[], 'img':['src', 'width', 'height', 'alt', 'title'] }
    
    attributes_with_urls = [ 'href', 'src' ]
    
    soup = BeautifulSoup(html)           
  
    # now strip HTML we don't like.  
    for tag in soup.findAll():  
        if tag.name.lower() in blacklist:  
            # blacklisted tags are removed in their entirety  
            tag.extract()  
        elif tag.name.lower() in whitelist:  # names are like 'a', 'p', etc.
            # tag is allowed. Make sure all the attributes are allowed.

            # NOTE: use an array to keep track of bad attr's, then remove them
            # from the tag.attrs dictionary AFTER the loop so no "change size" error
            bad_attrs = []
            for attr in tag.attrs: # attrs are like name, class, href, etc.
                # allowed attributes are whitelisted per-tag
                #print attr
                if tag.name.lower() in attr_whitelist and attr.lower() in attr_whitelist[ tag.name.lower() ]:
                    # Check for safe URLs. Don't want javascript, etc.
                    if attr.lower() in attributes_with_urls:
                        #print "!!!!!!!!!!!!!!!!!"
                        #print tag.attrs[attr]
                        #print "!!!!!!!!!!!!!!!!!!"
                        # TODO: support relative URLS, like "smiley.gif"? or do we care?
                        # TODO: if its like javascript.URL, should we just turn it into string
                        # or just remove it like it currently does b/c they want "text to come through"
                        if not re.match(r'(https?|ftp)://', tag.attrs[attr].lower()):
                            #print "~~~"
                            #print attr
                            bad_attrs += [attr] # TODO: fix and uncomment
                            # ok, then
                            pass
                else:
                    # Remove tags that are not whitelisted
                    # Note: includes 'class', 'id', 'name' atm as described above
                    bad_attrs += [attr]
            # Remove the bad attrs from the tag
            print "\n"
            print tag.name
            print tag.attrs
            print bad_attrs
            for ba in bad_attrs:
                del tag.attrs[ba]
        else:  
            # not a whitelisted tag. I'd like to remove it from the tree  
            # and replace it with its children. But that's hard. It's much  
            # easier to just replace it with an empty span tag.  
            tag.name = "span" 
            #tag.hidden = True #TODO: determine if we want it to be in a SPAN or just to hide the tag
            tag.attrs = []  
  
    # Just remove all comments just in case they have scripts in them
    comments = soup.findAll(text=lambda text:isinstance(text, Comment))  
    for comment in comments:  
        comment.extract()  
  
    safe_html = unicode(soup)  
    return safe_html


print sanitize(html_doc)
